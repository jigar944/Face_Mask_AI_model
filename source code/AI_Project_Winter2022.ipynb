{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "KIX5PCC4Vic9"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "from torchvision.datasets import ImageFolder\n",
    "from torch.utils.data import Dataset, random_split, DataLoader\n",
    "from torch import nn\n",
    "import torchvision.transforms as transforms \n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import accuracy_score,classification_report,confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ne9RtrZ7U1qZ",
    "outputId": "eac8e588-fac1-461b-d07a-2dec3d7f384d"
   },
   "outputs": [],
   "source": [
    "# Some predefine variable for different path\n",
    "\n",
    "# Modify below variable according to your need\n",
    "dataset_path = \"D:\\Study\\COMP6721\\Project\\project_final\\dataset\" # This contains path to the actual dataset\n",
    "model_path = \"D:\\Study\\COMP6721\\Project\\project_final\\model\\model.pth\" # This contains path to the actual model\n",
    "test_image_path = r\"D:\\Study\\COMP6721\\Project\\project_final\\test_image\" # This contains path for images to test, don't remove 'r'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "NpeQuVkrVWvV",
    "outputId": "e5224635-eb33-4939-cb85-2790f48f673c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of images: 2099\n",
      "number of classes: 5\n",
      "['Cloth mask', 'N-95 mask with valve', 'N-95_Mask', 'No Face Mask', 'Surgical Mask']\n"
     ]
    }
   ],
   "source": [
    "dataset = ImageFolder(dataset_path)\n",
    "print(f'number of images: {len(dataset)}')\n",
    "print(f'number of classes: {len(dataset.classes)}')\n",
    "target_names = dataset.classes\n",
    "print(target_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "G0IaUYgJJcmO",
    "outputId": "86f7ba29-9b71-4ce8-b75b-8bea7f971dae"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1575\n"
     ]
    }
   ],
   "source": [
    "# Here we are splitting our dataset into 1500 training and 500 testing images\n",
    "test_pct = 0.25\n",
    "test_size = int(len(dataset)*test_pct)\n",
    "train_size = len(dataset) - test_size\n",
    "train_ds, test_ds = random_split(dataset, [train_size, test_size])\n",
    "print(len(train_ds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "SZe3js8dK-7O"
   },
   "outputs": [],
   "source": [
    "class FaceMaskDataset(Dataset):\n",
    "    \n",
    "    def __init__(self, ds, transform=None):\n",
    "        self.ds = ds\n",
    "        self.transform = transform\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.ds)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        img, label = self.ds[idx]\n",
    "        if self.transform:\n",
    "            img = self.transform(img)  \n",
    "            return img, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "IbCJaD0iLFCX"
   },
   "outputs": [],
   "source": [
    "batch_size = 100\n",
    "\n",
    "train_transform = transforms.Compose([\n",
    "   transforms.Resize((50,50)),\n",
    "    transforms.ToTensor()    \n",
    "])\n",
    "\n",
    "\n",
    "test_transform = transforms.Compose([\n",
    "    transforms.Resize((50,50)), \n",
    "    transforms.ToTensor()\n",
    "])\n",
    "\n",
    "train_dataset = FaceMaskDataset(train_ds, train_transform)\n",
    "test_dataset = FaceMaskDataset(test_ds, test_transform)\n",
    "\n",
    "train_dl = DataLoader(train_dataset, batch_size, shuffle=True)\n",
    "test_dl = DataLoader(test_dataset, batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "wZkTpDbILeJC"
   },
   "outputs": [],
   "source": [
    "# This method is use to train model on 1500 images we have splitted\n",
    "def Train(model,optimizer,dataloader,device):\n",
    "    loss_tracker = []\n",
    "    Train_accuracy_tracker = []\n",
    "    correct = total = 0\n",
    "    for i,(data,label) in enumerate(dataloader):\n",
    "        data = data.requires_grad_()\n",
    "        data.to(device)\n",
    "        label = label.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        outputs= model(data)\n",
    "        CE_loss = nn.CrossEntropyLoss()\n",
    "        loss = CE_loss(outputs,label)\n",
    "        loss.backward()  \n",
    "\n",
    "        optimizer.step()\n",
    "\n",
    "        \n",
    "        with torch.no_grad():\n",
    "                _,predicted = torch.max(outputs.data,1)\n",
    "                total += label.size(0)\n",
    "                correct += (predicted == label).sum().item()\n",
    "                accuracy = (correct/total)*100\n",
    "        loss_tracker.append(loss.item())\n",
    "        Train_accuracy_tracker.append(accuracy)\n",
    "\n",
    "    return loss_tracker, Train_accuracy_tracker\n",
    "\n",
    "# Here we will test our input images and 500 testing images\n",
    "def Test(model,dataloader,device):\n",
    "    loss_tracker = []\n",
    "    Test_accuracy_tracker = []\n",
    "    total = correct = 0\n",
    "    predict = []\n",
    "    labels = []\n",
    "    for i,(data,label) in enumerate(dataloader):\n",
    "        data.to(device)\n",
    "        label.to(device)\n",
    "        labels.extend(label)\n",
    "        with torch.no_grad():\n",
    "         \n",
    "            output = model(data)\n",
    "            CE_loss = nn.CrossEntropyLoss()\n",
    "            loss = CE_loss(output,label)\n",
    "            _,predicted = torch.max(output.data,1)\n",
    "            predict.extend(predicted)\n",
    "            total += label.size(0)\n",
    "            correct += (predicted == label).sum().item()\n",
    "            accuracy = (correct/total)*100\n",
    "            \n",
    "        loss_tracker.append(loss.item())\n",
    "        Test_accuracy_tracker.append(accuracy)\n",
    "        \n",
    "    return sum(loss_tracker)/len(loss_tracker), sum(Test_accuracy_tracker)/len(Test_accuracy_tracker),labels,predict\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "asynjl0fNO00"
   },
   "outputs": [],
   "source": [
    "class conv_net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(conv_net,self).__init__()\n",
    "        self.network = nn.Sequential(\n",
    "                                      nn.Conv2d(3, 12, kernel_size=3, stride=1, padding=1,bias=True),\n",
    "                                      nn.ReLU(inplace=True),\n",
    "                                      nn.MaxPool2d(kernel_size=3, stride=2),\n",
    "                                     \n",
    "                                      nn.Conv2d(12, 24, kernel_size=3, stride=1, padding=1,bias=True), \n",
    "                                      nn.ReLU(inplace=True),\n",
    "                                      nn.MaxPool2d(kernel_size=3, stride=2),\n",
    "                                     \n",
    "                                      nn.Conv2d(24, 48, kernel_size=3, stride=1, padding=1,bias=True),\n",
    "                                      nn.ReLU(inplace=True),\n",
    "                                      nn.MaxPool2d(kernel_size=3, stride=2),\n",
    "                                     \n",
    "                                      nn.Dropout(p=0.3),\n",
    "                                      nn.Flatten(),\n",
    "                                      nn.Linear(48 * 5 * 5, 196),\n",
    "                                      nn.Linear(196, 5)\n",
    "                                    )\n",
    "        \n",
    "    \n",
    "    def forward(self,x):\n",
    "        return self.network(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "o-ZH7SMxO11R",
    "outputId": "31750834-6f19-43a8-d601-ff97d42bb355"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "device: cpu\n",
      "epoch: 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "E:\\Anaconda\\envs\\test\\lib\\site-packages\\PIL\\Image.py:945: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t training loss/accuracy: 1.59/19.16\n",
      "\t testing loss/accuracy: 1.56/24.26\n",
      "epoch: 1\n",
      "\t training loss/accuracy: 1.45/34.30\n",
      "\t testing loss/accuracy: 1.44/38.37\n",
      "epoch: 2\n",
      "\t training loss/accuracy: 1.32/43.57\n",
      "\t testing loss/accuracy: 1.36/43.47\n",
      "epoch: 3\n",
      "\t training loss/accuracy: 1.22/50.61\n",
      "\t testing loss/accuracy: 1.35/46.73\n",
      "epoch: 4\n",
      "\t training loss/accuracy: 1.18/51.42\n",
      "\t testing loss/accuracy: 1.25/48.16\n",
      "epoch: 5\n",
      "\t training loss/accuracy: 1.13/54.92\n",
      "\t testing loss/accuracy: 1.25/48.80\n",
      "epoch: 6\n",
      "\t training loss/accuracy: 1.08/53.90\n",
      "\t testing loss/accuracy: 1.19/52.42\n",
      "epoch: 7\n",
      "\t training loss/accuracy: 1.04/55.60\n",
      "\t testing loss/accuracy: 1.16/51.85\n",
      "epoch: 8\n",
      "\t training loss/accuracy: 0.97/60.68\n",
      "\t testing loss/accuracy: 1.11/56.29\n",
      "epoch: 9\n",
      "\t training loss/accuracy: 0.94/60.06\n",
      "\t testing loss/accuracy: 1.10/55.50\n",
      "epoch: 10\n",
      "\t training loss/accuracy: 0.92/63.87\n",
      "\t testing loss/accuracy: 1.07/53.01\n",
      "epoch: 11\n",
      "\t training loss/accuracy: 0.88/64.97\n",
      "\t testing loss/accuracy: 1.03/58.46\n",
      "epoch: 12\n",
      "\t training loss/accuracy: 0.86/66.23\n",
      "\t testing loss/accuracy: 1.07/56.90\n",
      "epoch: 13\n",
      "\t training loss/accuracy: 0.83/65.34\n",
      "\t testing loss/accuracy: 1.09/57.68\n",
      "epoch: 14\n",
      "\t training loss/accuracy: 0.83/65.67\n",
      "\t testing loss/accuracy: 1.00/59.87\n",
      "epoch: 15\n",
      "\t training loss/accuracy: 0.78/70.30\n",
      "\t testing loss/accuracy: 1.02/56.25\n",
      "epoch: 16\n",
      "\t training loss/accuracy: 0.77/69.46\n",
      "\t testing loss/accuracy: 1.03/56.00\n",
      "epoch: 17\n",
      "\t training loss/accuracy: 0.77/70.89\n",
      "\t testing loss/accuracy: 1.05/60.46\n",
      "epoch: 18\n",
      "\t training loss/accuracy: 0.74/74.67\n",
      "\t testing loss/accuracy: 0.98/62.02\n",
      "epoch: 19\n",
      "\t training loss/accuracy: 0.69/75.55\n",
      "\t testing loss/accuracy: 0.97/59.69\n",
      "epoch: 20\n",
      "\t training loss/accuracy: 0.68/74.78\n",
      "\t testing loss/accuracy: 0.99/59.93\n",
      "epoch: 21\n",
      "\t training loss/accuracy: 0.68/75.62\n",
      "\t testing loss/accuracy: 0.96/60.71\n",
      "epoch: 22\n",
      "\t training loss/accuracy: 0.62/75.52\n",
      "\t testing loss/accuracy: 0.98/61.80\n",
      "epoch: 23\n",
      "\t training loss/accuracy: 0.61/76.50\n",
      "\t testing loss/accuracy: 0.99/63.42\n",
      "epoch: 24\n",
      "\t training loss/accuracy: 0.62/77.32\n",
      "\t testing loss/accuracy: 1.07/57.69\n",
      "epoch: 25\n",
      "\t training loss/accuracy: 0.57/80.12\n",
      "\t testing loss/accuracy: 1.01/61.40\n",
      "epoch: 26\n",
      "\t training loss/accuracy: 0.55/78.58\n",
      "\t testing loss/accuracy: 1.03/62.94\n",
      "epoch: 27\n",
      "\t training loss/accuracy: 0.53/81.15\n",
      "\t testing loss/accuracy: 1.06/60.94\n",
      "epoch: 28\n",
      "\t training loss/accuracy: 0.52/80.53\n",
      "\t testing loss/accuracy: 1.10/60.01\n",
      "epoch: 29\n",
      "\t training loss/accuracy: 0.58/77.33\n",
      "\t testing loss/accuracy: 0.94/60.66\n",
      "epoch: 30\n",
      "\t training loss/accuracy: 0.53/80.17\n",
      "\t testing loss/accuracy: 1.04/60.23\n",
      "epoch: 31\n",
      "\t training loss/accuracy: 0.51/79.48\n",
      "\t testing loss/accuracy: 0.98/64.62\n",
      "epoch: 32\n",
      "\t training loss/accuracy: 0.47/81.69\n",
      "\t testing loss/accuracy: 1.12/62.11\n",
      "epoch: 33\n",
      "\t training loss/accuracy: 0.44/83.25\n",
      "\t testing loss/accuracy: 1.02/61.45\n",
      "epoch: 34\n",
      "\t training loss/accuracy: 0.43/83.08\n",
      "\t testing loss/accuracy: 1.01/63.37\n",
      "epoch: 35\n",
      "\t training loss/accuracy: 0.46/81.86\n",
      "\t testing loss/accuracy: 1.00/66.56\n",
      "epoch: 36\n",
      "\t training loss/accuracy: 0.43/84.55\n",
      "\t testing loss/accuracy: 1.05/65.29\n",
      "epoch: 37\n",
      "\t training loss/accuracy: 0.40/86.04\n",
      "\t testing loss/accuracy: 1.02/66.26\n",
      "epoch: 38\n",
      "\t training loss/accuracy: 0.38/86.71\n",
      "\t testing loss/accuracy: 1.07/65.26\n",
      "epoch: 39\n",
      "\t training loss/accuracy: 0.41/85.31\n",
      "\t testing loss/accuracy: 1.04/63.70\n",
      "epoch: 40\n",
      "\t training loss/accuracy: 0.39/85.62\n",
      "\t testing loss/accuracy: 1.18/62.21\n",
      "epoch: 41\n",
      "\t training loss/accuracy: 0.37/86.59\n",
      "\t testing loss/accuracy: 1.09/64.99\n",
      "epoch: 42\n",
      "\t training loss/accuracy: 0.35/89.62\n",
      "\t testing loss/accuracy: 1.15/65.23\n",
      "epoch: 43\n",
      "\t training loss/accuracy: 0.34/88.01\n",
      "\t testing loss/accuracy: 1.09/65.83\n",
      "epoch: 44\n",
      "\t training loss/accuracy: 0.32/88.48\n",
      "\t testing loss/accuracy: 1.16/65.31\n",
      "epoch: 45\n",
      "\t training loss/accuracy: 0.32/88.93\n",
      "\t testing loss/accuracy: 1.15/66.38\n",
      "epoch: 46\n",
      "\t training loss/accuracy: 0.30/90.10\n",
      "\t testing loss/accuracy: 1.17/62.66\n",
      "epoch: 47\n",
      "\t training loss/accuracy: 0.29/88.29\n",
      "\t testing loss/accuracy: 1.15/67.65\n",
      "epoch: 48\n",
      "\t training loss/accuracy: 0.29/89.12\n",
      "\t testing loss/accuracy: 1.34/63.40\n",
      "epoch: 49\n",
      "\t training loss/accuracy: 0.29/88.32\n",
      "\t testing loss/accuracy: 1.22/66.80\n"
     ]
    }
   ],
   "source": [
    "epochs = 50\n",
    "learning_rate = 0.001\n",
    "\n",
    "device = \"cpu\"\n",
    "print(f'device: {device}')\n",
    "\n",
    "\n",
    "model = conv_net()\n",
    "model.to(device)\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), learning_rate)\n",
    "\n",
    "train_loss_tracker = []\n",
    "train_accuracy_tracker = []\n",
    "\n",
    "test_loss_tracker = []\n",
    "test_accuracy_tracker = []\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    print(f'epoch: {epoch}')\n",
    "    train_loss,train_accuracy = Train(model,optimizer,train_dl,device)\n",
    "    test_loss , test_accuracy,labels,predict = Test(model,test_dl,device)\n",
    "    train_loss_tracker.extend(train_loss)\n",
    "    train_accuracy_tracker.extend(train_accuracy)\n",
    "    test_loss_tracker.append(test_loss)\n",
    "    test_accuracy_tracker.append(test_accuracy)\n",
    "    \n",
    "    print('\\t training loss/accuracy: {0:.2f}/{1:.2f}'.format(sum(train_loss)/len(train_loss), sum(train_accuracy)/len(train_accuracy)))\n",
    "    print('\\t testing loss/accuracy: {0:.2f}/{1:.2f}'.format(test_loss, test_accuracy))\n",
    "\n",
    "torch.save(model, model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "H9ZQQdwBTSZi",
    "outputId": "e11c4516-45bc-45df-fcd5-fb04cd2450b5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                      precision    recall  f1-score   support\n",
      "\n",
      "          Cloth mask       0.65      0.66      0.66       104\n",
      "N-95 mask with valve       0.54      0.52      0.53       103\n",
      "           N-95_Mask       0.65      0.64      0.65       106\n",
      "        No Face Mask       0.88      0.81      0.84       109\n",
      "       Surgical Mask       0.68      0.76      0.72       102\n",
      "\n",
      "            accuracy                           0.68       524\n",
      "           macro avg       0.68      0.68      0.68       524\n",
      "        weighted avg       0.68      0.68      0.68       524\n",
      "\n",
      "[[69 19  4  5  7]\n",
      " [15 54 19  1 14]\n",
      " [ 5 16 68  4 13]\n",
      " [12  4  3 88  2]\n",
      " [ 5  7 10  2 78]]\n"
     ]
    }
   ],
   "source": [
    "# Classification report and Confusion matrix for our dataset and trained model\n",
    "print(classification_report(labels,predict, target_names=target_names))\n",
    "print(confusion_matrix(labels,predict))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 779
    },
    "id": "qwXNE5oP_Smo",
    "outputId": "f07333b9-d95f-4101-dcc8-4f978d8b69ce"
   },
   "outputs": [],
   "source": [
    "# Testing part for pre trained model\n",
    "gen_model = torch.load(model_path) \n",
    "test_image_path = test_image_path.replace(\"\\\\\",\"/\")\n",
    "image = ImageFolder(test_image_path)\n",
    "test_image = FaceMaskDataset(image, test_transform)\n",
    "test_data = DataLoader(test_image, 10)\n",
    "test_loss , test_accuracy,l,p = Test(gen_model,test_data,device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                      precision    recall  f1-score   support\n",
      "\n",
      "          Cloth mask       0.50      0.61      0.55        23\n",
      "N-95 mask with valve       0.59      0.59      0.59        22\n",
      "           N-95_Mask       0.79      0.52      0.63        21\n",
      "        No Face Mask       0.80      0.76      0.78        21\n",
      "       Surgical Mask       0.76      0.86      0.81        22\n",
      "\n",
      "            accuracy                           0.67       109\n",
      "           macro avg       0.69      0.67      0.67       109\n",
      "        weighted avg       0.68      0.67      0.67       109\n",
      "\n",
      "[[14  4  2  0  3]\n",
      " [ 6 13  1  1  1]\n",
      " [ 2  3 11  3  2]\n",
      " [ 3  2  0 16  0]\n",
      " [ 3  0  0  0 19]]\n"
     ]
    }
   ],
   "source": [
    "# Classification report and Confusion matrix for testing images\n",
    "print(classification_report(l,p, target_names=target_names))\n",
    "print(confusion_matrix(l,p))"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "AI_Project_Winter2022.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python [conda env:test]",
   "language": "python",
   "name": "conda-env-test-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
